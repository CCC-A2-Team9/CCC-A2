{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# team: Â  group 9\n",
    "# member:\n",
    "# Zhaoxiang NING, 1261076, Luoyang, China\n",
    "# Xinyi ZHOU, 1281911, Wuxi, China\n",
    "# Haochu WANG, 1281962, Nantong, China\n",
    "# Gengchang XU, 1214774, Zhuhai, China\n",
    "# Shiming ZHENG, 1149897, Melbourne, Australia\n",
    "import couchdb\n",
    "import vaderSentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import json\n",
    "import shapefile\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry.polygon import Polygon\n",
    "import re\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('C:/Users/lenovo/Desktop/CCC-Ass2/CCC-A2/config.ini')\n",
    "couch = couchdb.Server(\"http://admin:82419626@127.0.0.1:5984\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = couch.create('cccdemo1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ready to analyze the data\n",
    "file=shapefile.Reader('SA2_2016_AUST.shp')\n",
    "shapes=file.shapes()\n",
    "# records=file.records()\n",
    "suburbId = [692, 694, 695, 696, 698, 699, 700, 701, 703, 707, 714]\n",
    "suburbName = [\"Flemington\", \"Carlton\", \"Docklands\", \"East Melbourne\", \"Kensington\", \"Melbourne\", \"North Melbourne\", \"Parkville\", \"Southbank\", \"Port Melbourne\", \"South Yarra - East\"]\n",
    "sub_plon = []\n",
    "dic = {\"Flemington\":{\"pos\":0,\"neg\":0,\"neu\":0}, \"Carlton\":{\"pos\":0,\"neg\":0,\"neu\":0}, \"Docklands\":{\"pos\":0,\"neg\":0,\"neu\":0}, \"East Melbourne\":{\"pos\":0,\"neg\":0,\"neu\":0}, \"Kensington\":{\"pos\":0,\"neg\":0,\"neu\":0}, \"Melbourne\":{\"pos\":0,\"neg\":0,\"neu\":0}, \"North Melbourne\":{\"pos\":0,\"neg\":0,\"neu\":0}, \"Parkville\":{\"pos\":0,\"neg\":0,\"neu\":0}, \"Southbank\":{\"pos\":0,\"neg\":0,\"neu\":0}, \"Port Melbourne\":{\"pos\":0,\"neg\":0,\"neu\":0}, \"South Yarra - East\":{\"pos\":0,\"neg\":0,\"neu\":0}}\n",
    "dic2 = {\"Flemington\":{\"pos\":0,\"neg\":0,\"neu\":0}, \"Carlton\":{\"pos\":0,\"neg\":0,\"neu\":0}, \"Docklands\":{\"pos\":0,\"neg\":0,\"neu\":0}, \"East Melbourne\":{\"pos\":0,\"neg\":0,\"neu\":0}, \"Kensington\":{\"pos\":0,\"neg\":0,\"neu\":0}, \"Melbourne\":{\"pos\":0,\"neg\":0,\"neu\":0}, \"North Melbourne\":{\"pos\":0,\"neg\":0,\"neu\":0}, \"Parkville\":{\"pos\":0,\"neg\":0,\"neu\":0}, \"Southbank\":{\"pos\":0,\"neg\":0,\"neu\":0}, \"Port Melbourne\":{\"pos\":0,\"neg\":0,\"neu\":0}, \"South Yarra - East\":{\"pos\":0,\"neg\":0,\"neu\":0}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct suburb boundary\n",
    "for i in range(len(shapes)):\n",
    "    # Using the points information to draw the ploygon\n",
    "    if i in suburbId:\n",
    "        polygon = Polygon(shapes[i].points)\n",
    "        sub_plon.append(polygon)\n",
    "sub_info = list(zip(suburbName,sub_plon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence analyzer\n",
    "def sentimentanlyzer(sentence):\n",
    "    sentiment = analyzer.polarity_scores(sentence)\n",
    "    compound = sentiment['compound']\n",
    "    if(compound>0.05):\n",
    "        ## positive\n",
    "        return \"pos\"\n",
    "    elif(compound<=-0.05):\n",
    "        ##negative\n",
    "        return \"neg\"\n",
    "    else:\n",
    "        ##neutral\n",
    "        return \"neu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test for finding keywords\n",
    "# with open(\"E:/90024/Assignment2/twitter-melb.json\", \"rb\") as f:\n",
    "#     counter = 0\n",
    "#     total_line = 50000\n",
    "\n",
    "#     for line in f:\n",
    "#         if 0 < counter < total_line-2:\n",
    "#             tweet = json.loads(line[0:len(line) - 3])\n",
    "#             content = tweet['doc']['text']\n",
    "#             coord = tweet['doc']['coordinates']\n",
    "#             if(coord):\n",
    "#                 if (\"education\" in content):\n",
    "#                     print(content)\n",
    "#             else:\n",
    "#                 counter += 1\n",
    "#                 continue\n",
    "#         counter += 1\n",
    "#     print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwordslist(filepath):\n",
    "    stopwords = [line.strip() for line in open(filepath, 'r', encoding='utf-8').readlines()]\n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwordsL = stopwordslist(\"C:/Users/lenovo/Desktop/stopwords.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)\n",
    "        \n",
    "    return input_txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500002\n"
     ]
    }
   ],
   "source": [
    "# ******  The path should be changed to relative path  ******\n",
    "# Case 2\n",
    "with open(\"E:/90024/Assignment2/twitter-melb.json\", \"rb\") as f:\n",
    "    counter = 0\n",
    "    total_line = 2500000\n",
    "\n",
    "    for line in f:\n",
    "        if 0 < counter < total_line-2:\n",
    "            tweet = json.loads(line[0:len(line) - 3])\n",
    "            content = tweet['doc']['text'].lower()\n",
    "            coord = tweet['doc']['coordinates']\n",
    "            if(coord):\n",
    "                coordinate = tweet['doc']['coordinates']['coordinates']\n",
    "                words = [word for word in content.split() if word.lower() not in stopwordsL]\n",
    "                content = \" \".join(words)\n",
    "                if (\"education\" in content) or (\"school\" in content) or (\"teacher\" in content) or (\"teachers\" in content) or (\"student\" in content) or (\"students\" in content) or (\"study\" in content):\n",
    "                    content = re.split('https:\\/\\/.*|http:\\/\\/.*',content)[0]\n",
    "                    content = remove_pattern(content, \"@[\\w]*\")\n",
    "                    sentiment = sentimentanlyzer(content)\n",
    "                    point = Point(coordinate[0],coordinate[1])\n",
    "                    for sub in sub_info:\n",
    "                        # Return true if the point is in the ploygon\n",
    "                        if sub[1].contains(point):\n",
    "                            # Collect the result\n",
    "                            dic[sub[0]][sentiment] +=1\n",
    "                            break\n",
    "            else:\n",
    "                counter += 1\n",
    "                continue\n",
    "\n",
    "        counter += 1\n",
    "    print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500002\n"
     ]
    }
   ],
   "source": [
    "# Case 1\n",
    "with open(\"E:/90024/Assignment2/twitter-melb.json\", \"rb\") as f:\n",
    "    counter = 0\n",
    "    total_line = 2500000\n",
    "\n",
    "    for line in f:\n",
    "        if 0 < counter < total_line-2:\n",
    "            tweet = json.loads(line[0:len(line) - 3])\n",
    "            content = tweet['doc']['text'].lower()\n",
    "            coord = tweet['doc']['coordinates']\n",
    "            tweetid = tweet['doc']['_id']\n",
    "            if(coord):\n",
    "                coordinate = tweet['doc']['coordinates']['coordinates']\n",
    "                words = [word for word in content.split() if word.lower() not in stopwordsL]\n",
    "                content = \" \".join(words)\n",
    "                if (\"job\" in content) or (\"employment\" in content) or (\"recruit\" in content) or (\"recruitment\" in content) or (\"unemployment\" in content) or (\"salary\" in content):\n",
    "                    content = re.split('https:\\/\\/.*|http:\\/\\/.*',content)[0]\n",
    "                    content = remove_pattern(content, \"@[\\w]*\")\n",
    "                    sentiment = sentimentanlyzer(content)\n",
    "                    point = Point(coordinate[0],coordinate[1])\n",
    "                    for sub in sub_info:\n",
    "                        # Return true if the point is in the ploygon\n",
    "                        if sub[1].contains(point):\n",
    "                            # Collect the result\n",
    "                            doc1 = {\"id\":tweetid, \"suburb\":sub[0], \"emotion\":sentiment}\n",
    "                            db.save(doc1)\n",
    "                            dic2[sub[0]][sentiment] +=1\n",
    "                            break\n",
    "            else:\n",
    "                counter += 1\n",
    "                continue\n",
    "\n",
    "        counter += 1\n",
    "    print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ViewResults <PermanentView '_design/users/_view/tweetView'> {}>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# def fill_data(users_no):\n",
    "#     for i in range(users_no):\n",
    "#         doc = {\n",
    "#             '_id': str(i),\n",
    "#             'uname': \"name_\" + str(i),\n",
    "#         }\n",
    "#         db.save(doc)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    server = couchdb.Server(\"http://admin:82419626@127.0.0.1:5984\")\n",
    "    db = server[\"cccdemo1\"]\n",
    "\n",
    "    map_fun = \"\"\"\n",
    "                function (doc) {\n",
    "                  emit([doc.suburb,doc.emotion],1);\n",
    "                }\n",
    "              \"\"\"\n",
    "    reduce_fun =\"_count\"\n",
    "\n",
    "\n",
    "    design = { 'views': {\n",
    "              'tweetView': {\n",
    "                  'map': map_fun,\n",
    "                  'reduce': reduce_fun\n",
    "                }\n",
    "            } }\n",
    "    db[\"_design/users\"] = design\n",
    "\n",
    "    uname_list = db.view('users/tweetView')\n",
    "\n",
    "    print(uname_list)\n",
    "    for r in db.view('users/tweetView') :\n",
    "        print(r.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'couchdb.client.ViewResults'>\n"
     ]
    }
   ],
   "source": [
    "print(type(uname_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Flemington': {'pos': 5, 'neg': 5, 'neu': 1},\n",
       " 'Carlton': {'pos': 24, 'neg': 5, 'neu': 43},\n",
       " 'Docklands': {'pos': 13, 'neg': 11, 'neu': 21},\n",
       " 'East Melbourne': {'pos': 34, 'neg': 9, 'neu': 35},\n",
       " 'Kensington': {'pos': 5, 'neg': 5, 'neu': 2},\n",
       " 'Melbourne': {'pos': 157, 'neg': 59, 'neu': 183},\n",
       " 'North Melbourne': {'pos': 14, 'neg': 9, 'neu': 20},\n",
       " 'Parkville': {'pos': 29, 'neg': 7, 'neu': 47},\n",
       " 'Southbank': {'pos': 50, 'neg': 9, 'neu': 54},\n",
       " 'Port Melbourne': {'pos': 4, 'neg': 2, 'neu': 4},\n",
       " 'South Yarra - East': {'pos': 15, 'neg': 6, 'neu': 24}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Flemington': {'pos': 0, 'neg': 0, 'neu': 0},\n",
       " 'Carlton': {'pos': 16, 'neg': 1, 'neu': 37},\n",
       " 'Docklands': {'pos': 39, 'neg': 3, 'neu': 37},\n",
       " 'East Melbourne': {'pos': 31, 'neg': 7, 'neu': 15},\n",
       " 'Kensington': {'pos': 5, 'neg': 3, 'neu': 0},\n",
       " 'Melbourne': {'pos': 751, 'neg': 48, 'neu': 1409},\n",
       " 'North Melbourne': {'pos': 9, 'neg': 5, 'neu': 8},\n",
       " 'Parkville': {'pos': 5, 'neg': 4, 'neu': 3},\n",
       " 'Southbank': {'pos': 30, 'neg': 12, 'neu': 26},\n",
       " 'Port Melbourne': {'pos': 4, 'neg': 3, 'neu': 3},\n",
       " 'South Yarra - East': {'pos': 13, 'neg': 7, 'neu': 7}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert dic to json file\n",
    "dict_json=json.dumps(dic)\n",
    "with open('file.json','w+') as file:\n",
    "    file.write(dict_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the word cloud analyzer\n",
    "f = open(u'E:/90024/Assignment2/processed3.txt','r',encoding=\"utf-8\").read()\n",
    "f = f.lower()\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
